## Summary
The goal is to predict the manner in which they did the exercise. 
This is the "classe" variable in the training set. 

## Exploratory analysis
As first step load and take a look to the dataset
```{r load, cache=FALSE, echo=FALSE}
harData= read.csv("pml-training.csv")
predictData=read.csv("pml-testing.csv")
str(harData)
```
There are 19622 cases and 159 potential predictors , so we should proceed with some data cleaning and feature reduction, given lot of them seems to be NA.
```{r setup, cache=FALSE, echo=FALSE}
library(caret)
library(randomForest)
set.seed(123)
har.Sample=harData
```
### Cleanup and Feature selection
Looking at the sample, some consideration can be done. We could cleanup the variable clearly numeric that has been intrepreted as factor and the "auxiliary" variables, like the ones timestamp and names of the people 

**possible assumption:** having a variable containing few distinct real number and not a distribution of them is an index of poor quality of data or that the variable is not meanigful. Ex: kurtosis_roll_belt  
Using  the *nearZeroVar* of caret package would do the job of removing such low-variation variables.  

In order to allow for the predicion algorithhm to be applied also to the test data, the columns that in TEST data contains NA are removed also from te Training data. So, to fit the model we will use only the predictors that contains actually meaningful values in the test data. Note that this is not equal to using test data to train the model. We are only choosing which predictor shoudl be contained in the model

```{r cleanup, cache=FALSE, echo=FALSE}
nearZeroPredictVar=nearZeroVar( predictData,saveMetrics=TRUE)
#use the TEST DATA instead to detect NZval
columnsValid=   names(predictData)[nearZeroPredictVar$nzv==FALSE]
# start cleanup of the TRAINING data using the columns that are Non zero in the test data 
har.Sample.Clean=har.Sample
# build here the  classe as factor
classeF=as.factor(har.Sample$classe)
columnsFactor= columnsValid;
# now cleanup also the other non meaningful variables like name and timestamps 
# and the problem_id, that exist only in the test data
columnsFactor=columnsFactor[columnsFactor!="X" & columnsFactor!="raw_timestamp_part_1" & columnsFactor!=   "raw_timestamp_part_2" & columnsFactor!="cvtd_timestamp" & columnsFactor!="user_name" 
                            &  columnsFactor!="problem_id"   ]
har.Sample.Clean=har.Sample.Clean[,columnsFactor]
har.Sample.Clean$classe=classeF
```
After cleanup and removal of the timestamp and names variables we have `r length(names(har.Sample.Clean))` variables left. Perform usual train & set data separation  

```{r samples, cache=FALSE, echo=FALSE}
inTrain=createDataPartition(har.Sample.Clean$classe, p=.7, list=FALSE)
training.Sample=har.Sample.Clean[inTrain,]
test.Sample= har.Sample.Clean[-inTrain,]
```

## Fitting Data
  
Jump directly on the cart application of the tree, to get some insight. Ignore the NA and retreieve the importance of the variables for further consideration

```{r fit_rf, cache=TRUE, echo=TRUE}
har.sample.rf = randomForest(classe ~ ., data =training.Sample,importance=TRUE,na.action=na.omit)
har.sample.rf
summary(har.sample.rf$err.rate)
```
Cross validation is done automatically by the randomForest fucntion(see documentation)  
A summarized OOB error is in the summary below. As furhter detail is shown the *err.rate* , that gives the vector error rates of the prediction on the input data, the i-th
element being the (OOB) error rate for all trees up to the i-th  
Predict the values on the test sample and check manually the Accuracy.
```{r predict_rf, cache=FALSE, echo=TRUE}
har.sample.predict = predict(har.sample.rf, newdata=test.Sample, type="response")
t=table(predicted=har.sample.predict,observed=test.Sample[,"classe"])
```

```{r summary, cache=FALSE, echo=FALSE}
t
s=0
for(i in 1:nrow(t)) {s= s+ t[i,i]}
tot=sum(t)
```
Accuracy is `r s/tot*100`%
  
Search the be the most meaningful variables to be taken into account, using importance. 
```{r var_select, cache=FALSE, echo=TRUE}
har.sample.rf.importance =data.frame(importance(har.sample.rf))
#An initial examination is done using the summary on the importance , detecting the value 450 as threshold for Gini index.
summary(har.sample.rf.importance)
```
The most important varaibles, using a threshold of 450 (in the 3rd quantile of the str result) are the variables:  
```{r var_importance, cache=FALSE, echo=TRUE}
har.sample.rf.importance[which(har.sample.rf.importance$MeanDecreaseGini>450),]
```