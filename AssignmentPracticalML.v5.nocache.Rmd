## Summary
The goal is to predict the manner in which they did the exercise. 
This is the "classe" variable in the training set. 

## Exploratory analysis
As first step load and take a look to the dataset
```{r load, cache=FALSE, echo=FALSE}
harData= read.csv("pml-training.csv")
predictData=read.csv("pml-testing.csv")
str(harData)
```
There are 19622 cases and 159 potential predictors , so we should proceed with some data cleaning and feature reduction, given lot of them seems to be NA.
```{r setup, cache=FALSE, echo=FALSE}
library(caret)
library(randomForest)
set.seed(123)
har.Sample=harData
```
### Cleanup and Feature selection
Looking at the sample, some consideration can be done. We could cleanup the variable clearly numeric that has been intrepreted as factor and the "auxiliary" variables, like the ones timestamp and names of the people 

**possible assumption:** having a variable containing few distinct real number and not a distribution of them is an index of poor quality of data or that the variable is not meanigful. Ex: kurtosis_roll_belt  
Using  the *nearZeroVar* of caret package would do the job of removing such low-variation variables.

```{r cleanup, cache=FALSE, echo=FALSE}

# save the classe data as factor
nearZero=nearZeroVar( har.Sample,saveMetrics = TRUE)
har.Sample.Clean=har.Sample[,nearZero$nzv==FALSE]
##build here the cleaned class factor
classeF=as.factor(har.Sample.Clean$classe)
#remove also the various timestamp
columnsFactor= names(har.Sample.Clean);
columnsFactor=columnsFactor[columnsFactor!="X" & columnsFactor!="raw_timestamp_part_1" & columnsFactor!=   "raw_timestamp_part_2" & columnsFactor!="cvtd_timestamp" & columnsFactor!="user_name"]
har.Sample.Clean=har.Sample.Clean[,columnsFactor]
har.Sample.Clean$classe=classeF
```
After cleanup and removal of the timestamp and names variables we have `r length(names(har.Sample.Clean))` variables left. Perform usual train & set data separation  

```{r samples, cache=FALSE, echo=FALSE}
inTrain=createDataPartition(har.Sample.Clean$classe, p=.7, list=FALSE)
training.Sample=har.Sample.Clean[inTrain,]
test.Sample= har.Sample.Clean[-inTrain,]
```

## Fitting Data
  
Jump directly on the cart application of the tree, to get some insight. Ignore the NA and rereieve the importance of the variables for further consideration

```{r fit_rf, cache=FALSE, echo=TRUE}
har.sample.rf = randomForest(classe ~ ., data =training.Sample,importance=TRUE,na.action=na.omit)
har.sample.rf
summary(har.sample.rf$err.rate)
```
Cross validation is done automatically by the randomForest fucntion(see documentation)
The *err.rate* gives the vector error rates of the prediction on the input data, the i-th
element being the (OOB) error rate for all trees up to the i-th  
Predict the values on the test sample and check manually the accuracy.
```{r predict_rf, cache=FALSE, echo=TRUE}
har.sample.predict = predict(har.sample.rf, newdata=test.Sample, type="response")
t=table(predicted=har.sample.predict,observed=test.Sample[,"classe"])
```

```{r summary, cache=FALSE, echo=FALSE}
t
s=0
for(i in 1:nrow(t)) {s= s+ t[i,i]}
tot=sum(t)
```
Accuracy is `r s/tot*100`%

## Additional Exploration

Search the be the most meaningful variables to be taken into account, using importance 
```{r var_select, cache=FALSE, echo=TRUE}
har.sample.rf.importance =data.frame(importance(har.sample.rf))
har.sample.rf.importance[which(har.sample.rf.importance$MeanDecreaseGini>5.5),]
```
Try to use these variables:  *`r names(har.Sample.Clean)[which(har.sample.rf.importance$MeanDecreaseGini>5.5)]`*   
to build a linear model or just check if the relation could be meaningful
```{r fit_glm,cache=FALSE,echo=TRUE}
har.sample.lr=glm(classe~ avg_roll_belt+ stddev_roll_belt+ var_roll_belt+ var_accel_dumbbell+ avg_roll_dumbbell , data=training.Sample, family="binomial" ,na.action=na.omit)
summary(har.sample.lr)
```
There is an high percentage of observation deleted is not reassuring. Only in one case a meaningful Z score appears
showing a relevant significance.  
Plot it against the *classe* for the sake of curiosity  
```{r,fig.width=6, fig.height=6, echo=FALSE}
boxplot(training.Sample$var_accel_dumbbell~training.Sample$classe, xlab="classe" ,ylab="var_accel_dumbbell")
```

  

